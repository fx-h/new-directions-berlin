---
title: | 
    | Data Science for Causal Inference
author: "Ryan T. Moore"
date: 2024-07-15
date-format: iso
execute: 
  echo: true
format: 
  beamer:
    fonttheme: serif
    include-in-header:
      - text: | 
          \usepackage{wasysym} 
    section-titles: true
    toc: true
institute:
  - American University
  - The Lab @ DC
bibliography: "../admin/main.bib"
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| results: false
#| warning: false

library(broom)
library(dagitty)
library(ggdag)
library(here)
library(tidyverse)
```

# Introductions

## About Me

\Large

- Associate Prof of Government  
(American University)
- Associate Director, Center for Data Science  
(American University)
- Senior Social Scientist  
(The Lab @ DC)
- Fellow in Methodology  
(US Office of Evaluation Sciences: "OES") \pause 
- Research agenda: political methodology, causal inference, experimental design, 
experiments in public policy

## About You!

\Large

>- Name?
>- Role? 
>- Interests?
>- Olympic sport you look forward to?

## Plan

\large

>- Data Science in Causal Inference
>    - Models
>    - Heterogeneous treatment effects
>    - Variable selection
>- Sensitivity
>    - Model specification
>    - Unobservable parameter
>    - Unobserved confounders
>- Modern difference-in-difference designs
>    - Canonical DiD
>    - Multiple time periods
>    - Staggered adoption
>    - Calloway-Sant'Anna approach



# Data Science in Causal Inference

## Causal Inference Approaches

The "potential outcomes" framework:

\pause 

\begin{center}
\begin{tabular}{ccccc}
 & & Would Enroll if & Would Enroll if  & \\
Citizen & Canvass? & Canvass? & No Canvass? & Enroll \\ \hline
1 & Yes & \only<2->{Yes} & \only<6->{\alert{(Yes)}} &  Yes \\
2 & Yes & \only<3->{Yes} & \only<6->{\alert{(No)}} &  Yes \\
3 & No & \only<6->{\alert{(Yes)}} & \only<4->{No} &  No \\
4 & No &\only<6->{\alert{(No)}} & \only<5->{No} &  No
\end{tabular}
\end{center}

## Causal Inference Approaches

The "potential outcomes" framework, more abstractly:

\begin{center}
\begin{tabular}{cccccc}
&&&&& True $\tau$ \\
Unit $i$ & Treatment $T$ & $Y(1)$ & $Y(0)$ & $Y^{\text{obs}}$ & $Y(1) - Y(0)$ \\ \hline
1 & 1 & 10 & \only<2->{\alert{(10)}} &  10 & \only<2->{0}\\
2 & 1 & 20 & \only<2->{\alert{(10)}} &  20 & \only<2->{10}\\
3 & 0 & \only<2->{\alert{(40)}} & 15 &  15 & \only<2->{25}\\
4 & 0 & \only<2->{\alert{(20)}} & 5 &  5 & \only<2->{15} \\ \hline
&&&& \only<3->{ATE $=\bar{\tau}=$} & \only<3->{$\frac{50}{4} = 12.5$} \\
&&&& \only<4->{$\widehat{ATE}=\hat{\bar{\tau}}=$} & \only<4->{$15 - 10 =5$} \\
\end{tabular}
\end{center}

## Causal Inference Approaches

\large

The "potential outcomes" framework, notation:

- Units indexed by $i$
- Treatment $T_i$ or $D_i$ or $Z_i$
- Outcome if treated $Y_i(1)$
- Outcome if control $Y_i(0)$
- True treatment effect $\tau_i = Y_i(1) - Y_i(0)$
- True average treatment effect $\bar{\tau} = \frac{1}{n} \sum_{i=1}^n \left(Y_i(1) - Y_i(0)\right)$
- Pre-treatment covariates $\mathbf X$

\pause 
(and we'll draw some DAG's, too)

## Data Science Approaches

\Large 

Three tasks of data science:

>- Description
>- Prediction
>- Causal Inference

\pause 
\vspace{1mm}

Models/algorithms central to all three.

\pause 
\vspace{1mm}

@herhsuhea19

## Data Science Approaches

\large 

Description

>- Identifying patterns, etc.
>- E.g., clustering to discover groups

## Data Science Approaches

\large 

Prediction

>- Components
>    - Inputs/outputs (predictors/outcomes, features/responses, \ldots)
>    - Mapping from inputs to outputs (linear model, decision tree, \ldots)
>    - Metric for evaluating mapping
>- With these, model machine learning does the work
>- E.g., regression, random forests, neural networks, \ldots

## Data Science Approaches

\large

Causal Inference

>- Potential outcomes/counterfactual/interventionist perspective
>- Requires _expertise_ different to description/prediction
>- Requires more than summary statistics, metrics, etc.
>- Requires some knowledge of causal structure
>    - Not all inputs treated same
>    - $T$ v. $\mathbf X$ -- very different!
>    - (the more knowledge, the better!)
>    - (alternative: solve fundamental problem of causal inference!)
>- E.g., experiments, observational causal designs, \ldots

## Causal Inference with Machine Learning

\pause 

![Don't do this.](figs/01-ml-ols-tweet.png)
\pause 

(OK, not "machine learning", perhaps, but _models_ at least \ldots)

## Causal Inference with Models

```{r}
#| echo: false
library(quartets)

data("causal_confounding")
df1 <- causal_confounding

data("causal_collider")
df2 <- causal_collider
```

Loaded two datasets:

```{r}
str(df1)
str(df2)
```

## Causal Inference with Models

```{r}
#| echo: false
#| layout-ncol: 2

p1 <- ggplot(df1, aes(exposure, outcome)) + 
  geom_point(aes(color = covariate)) + geom_smooth(method = "lm") +
  labs(title = "Data set 1")

p2 <- ggplot(df2, aes(exposure, outcome)) + 
  geom_point(aes(color = covariate)) + geom_smooth(method = "lm") +
  labs(title = "Data set 2")

p1
p2
```

## Causal Inference with Models

Model each

```{r}
lm_df1 <- lm(outcome ~ exposure, data = df1)
lm_df2 <- lm(outcome ~ exposure, data = df2)
```

```{r}
#| echo: false

tidy_df1 <- tidy(lm_df1) |> mutate(data = "df1", .before = "term") |> select(data:std.error)
tidy_df2 <- tidy(lm_df2) |> mutate(data = "df2", .before = "term") |> select(data:std.error)
tidy_df12 <- bind_rows(tidy_df1, tidy_df2)
tidy_df12
```

\pause 

>- Both cases: effect of exposure $\approx 1$. 
>- Is this good?
>- What if we adjust for covariate?

## Causal Inference with Models

```{r}
lm_df1_adj <- lm(outcome ~ exposure + covariate, data = df1)
lm_df2_adj <- lm(outcome ~ exposure + covariate, data = df2)
```

```{r}
#| echo: false

tidy_df1_adj <- tidy(lm_df1_adj) |> mutate(data = "df1", .before = "term") |>
  filter(term != "(Intercept)") |> select(data:std.error)
tidy_df2_adj <- tidy(lm_df2_adj) |> mutate(data = "df2", .before = "term") |> 
  filter(term != "(Intercept)") |> select(data:std.error)
tidy_df12_adj <- bind_rows(tidy_df1_adj, tidy_df2_adj)
tidy_df12_adj
```

>- Both cases: effect of exposure $\approx 0.5$. 
>- Is this good?
>- Which is correct? $\beta = 1$? $\beta = 0.5$?
>- _Should_ we adjust for covariate?

## Causal Inference with Models

\Large
There is nothing in the data that tells us. \pause \frownie

\pause 

Here are the true structures:

```{r quartetdags}
#| echo: false
#| layout-ncol: 3
coords <- list(x = c(T = 0, X = 1, Y = 2), 
               y = c(T = 1, X = 0, Y = 1.1))
dag_conf <- dagify(Y ~ T, T ~ X, Y ~ X, coords = coords) 
ggdag_conf <- dag_conf |> 
  ggdag_classic(size = 30) + theme_dag_blank() 

coords <- list(x = c(T = 0, X = 1, Y = 2), 
               y = c(T = 1, X = 0, Y = 1.1))
dag_coll <- dagify(Y ~ T, X ~ T, X ~ Y, coords = coords) 
ggdag_coll <- dag_coll |> 
  ggdag_classic(size = 30) + theme_dag_blank()

ggdag_conf
ggplot(NULL) + theme(panel.background = element_blank())
ggdag_coll
```

## Causal Inference with Models

When know structures, adjustment sets for unbiasedness differ:

- `df1`: confounding $\Rightarrow$ **adjust for $X$**
- `df2`: collider $\Rightarrow$ **do not adjust for $X$**

```{r}
g_conf <- dagitty("dag{ x -> y ; x <- c -> y }")
g_coll <- dagitty("dag{ x -> y ; x -> c <- y }")

adjustmentSets(g_conf, "x", "y")
adjustmentSets(g_coll, "x", "y")
```

## Causal Inference with Models

When know structures, adjustment sets for unbiasedness differ:

- `df1`: confounding $\Rightarrow$ **adjust for $X$**
- `df2`: collider $\Rightarrow$ **do not adjust for $X$**

```{r quartetdagadjustments}
#| echo: false
#| layout-ncol: 2

dag_conf |> ggdag_adjustment_set(exposure = "T", outcome = "Y")
dag_coll |> ggdag_adjustment_set(exposure = "T", outcome = "Y")
```

\pause 

(Data from @dagostino23)

## Causal Inference with Models

\large

>- Importance of identifying "pre-treatment covariates", "proper covariates"; doing "design before analysis"
>- Importance of experiments: strong knowledge about (part of) causal structure
>- Causal inference is critical to scientific questions, and separate from prediction
>- Though, methods from prediction can aid causal inference
>- (A perspective on "causal euphimisms": @hernan18)


## Approaches of Prediction and Causal Inference

\large

_Two Cultures_, [@breiman01-two]

- _Data Models_: our "social science modeling"
- _Algorithmic Models_: our "data science algorithms"

## Methods for Prediction and Causal Inference

\large

- Cross-validation
- Regression/Decision trees
- Random forests

@jamwithas21

## Cross-validation

\large

$k$-fold cross-validation

- Randomly partition data into $k$ groups
- Apply method to $k-1$ groups
- Use result to predict for left-out group
- Calculate $\text{MSE}_i = \frac{1}{n} \sum\limits_{i=1}^n \left(y_i - \hat{y}_i \right)^2$ 
- Calculate test error as average of the $k$ MSE's:

$$CV_{(k)} = \frac{1}{k} \sum\limits_{i=1}^k \text{MSE}_i$$

- Select model that minimises $CV_{(k)}$


## CV for Linear Model

```{r}
library(tidyverse)

## Make data

mk_data <- function(n = 90, n_folds = 10){
  
  df <- tibble(
    x1 = rnorm(n),
    x2 = rnorm(n), 
    x3 = rnorm(n), 
    y = 0.1 * x1 + 0.2 * x2 + 0.5 * x3 + rnorm(n),
    cv_fold = sample(rep(1:n_folds, (n / n_folds)))
  )
  
}

df <- mk_data()
```

## CV for Linear Model

```{r}
head(df)
```

\pause 

```{r}
table(df$cv_fold)
```


## CV for Linear Model

```{r}
cv_lm <- function(data, fmla){
  
  n_folds <- max(data$cv_fold)
  store_mses <- vector("numeric", length = n_folds)
  
  for(idx in 1:n_folds){
    
    df_train <- data |> filter(cv_fold != idx)
    df_test <- data |> filter(cv_fold == idx)
  
    lm_out <- lm(fmla, data = df_train)
  
    predictions <- predict(lm_out, newdata = df_test)
    
    store_mses[idx] <- mean((df_test$y - predictions)^2)}

  test_error_cv_k <- mean(store_mses)
  return(test_error_cv_k)
}
```

## CV for Linear Model

\large

```{r}
cv_lm(data = df, fmla = y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2 + x3)
```

## CV for Linear Model

```{r}
#| echo: false
#| fig-cap: "MSE always less (better) for 3-variable model."
#| fig-height: 6

set.seed(631537784)

n_iter <- 100

mses_2var <- mses_3var <- vector("numeric", length = 100)

for(idx in 1:n_iter){
  df <- mk_data()
  mses_2var[idx] <- cv_lm(df, y ~ x1 + x2)
  mses_3var[idx] <- cv_lm(df, y ~ x1 + x2 + x3)
}

mses <- tibble(mses_2var, 
               mses_3var, 
               difference = mses_3var - mses_2var)

ggplot(mses, aes(difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "(MSE 3-var model) - (MSE 2-var model). 100 data sets.")

# Long version:
# mses <- mses |> pivot_longer(cols = starts_with("mses"),
#                              names_to = "model") |>
#   mutate(model = case_when(model == "mses_2var" ~ "y ~ x1 + x2", TRUE ~ "y ~ x1 + x2 + x3"))

# ggplot(mses, aes(value, model)) + 
#  geom_violin(draw_quantiles = c(.25, .5, .75))
```



## Regression Trees

\large

- Partition predictor space into regions $R_1, R_2, \ldots, R_J$.
- If unit falls in region $R_j$, use average outcome in $R_j$ as predicted value: $\hat{y}_{R_j}$
- (For _decision_ on discrete outcome, count votes in $R_j$)
- Goal: minimise residual sum of squares (RSS), just like LS regression:

$$\sum\limits_{j=1}^J \sum\limits_{i \in R_j} \left(y_i - \hat{y}_{R_j}\right)$$

## Regression Trees

\large


How to define regions $R_j$?

\pause 

- Top-down, greedy recursive binary split
- At each step, find predictor and cut-point that minimise

$$\sum\limits_{i: x\in R_1(j,s)} \left(y_i - \hat{y}_{R_1(j,s)}\right)^2 +
\sum\limits_{i:x \in R_2(j,s)}  \left(y_i - \hat{y}_{R_2(j,s)}\right)^2$$

## Regression Trees

\Large

>- Overfitting is a potential problem 
>- Can we increase predictive quality by only using _part_ of a tree?
>- "Pruning"

## Regression Trees

Pruning

>- Build a large tree
>- Select the subtree that gives least prediction error (via cross-validation)
>- But, many possible subtrees, so penalise larger trees via $\alpha$
>- $\alpha$: penalty parameter
>- $|T|$: count of terminal nodes of $T$
>- $m$: terminal node index
>- Find subtree that minimises \pause 

$$\sum\limits_{m=1}^{|T|} \sum\limits_{i:x_i \in R_m} \left(y_i - \hat{y}_{R_m} \right)^2 + \alpha |T|$$
\pause 

Sum squared pred. error (plus penalty that grows with tree size) across units in region, then regions. 

## Regression Trees

\large

But, how to choose $\alpha$? (Use cross-validation.)

1. Build big tree on training data (with some minimum terminal node size) \pause 
2. For several values of $\alpha$, find best subtree. \pause 
3. Find best value of $\alpha$ via CV. Create $K$ folds. Then \pause \newline
3a. Do 1 and 2 on all but $k$th fold  \pause  
\newline 
3b. Predict for $k$th fold, calculate MSE for several values of $\alpha$  \pause  \newline
3c. Get avg MSE for each $\alpha$  \pause \newline
3d. Pick $\alpha$ to minimise MSE  \pause \newline
4. Using that $\alpha$, select best subtree from Step 2

## Example: Regression Tree

```{r}
library(qss)
library(rsample)
library(tree)

data("MPs")
mps <- MPs |> mutate(age = yod - yob,
                     is_labour = if_else(party == "labour", 1, 0),
                     is_london = if_else(region == "Greater London", 1, 0), 
                     is_winner = if_else(margin > 0, 1, 0)) |>
  select(ln.net, age, is_labour, is_london, is_winner) |> 
  na.omit()

set.seed(765076184)

mp_split <- initial_split(mps, prop = 0.7)

mp_train <- training(mp_split)
mp_test <- testing(mp_split)

tree_mp <- tree(ln.net ~ ., data = mp_train)
```

## Example: Regression Tree

```{r}
#| fig-cap: "The regression tree (for training data)"

plot(tree_mp)
text(tree_mp)
```

## Example: Regression Tree

\large
Would pruning help?

```{r}
#| fig-cap: "Subtree size 2 minimises SSR"
#| fig-height: 4
cv_mps <- cv.tree(tree_mp, K = 10)

plot(cv_mps$size, cv_mps$dev, type = "b")
```

\pause 
Yes. So prune at $|T| = 2$.

## Example: Regression Tree

```{r}
#| fig-cap: "The pruned tree"
#| fig-height: 3

prune_mps <- prune.tree(tree_mp, best = 2)

plot(prune_mps)
text(prune_mps)
```

## Example: Regression Tree

\large

Predict for test set:

```{r}
#| echo: false
mps_preds_full <- predict(tree_mp, newdata = mp_test)
mps_preds_prune <- predict(prune_mps, newdata = mp_test)

df_mp_preds <- tibble(
  preds_full = mps_preds_full,
  preds_prune = mps_preds_prune,
  observed = mp_test$ln.net
)

# pl_full <- ggplot(df_mp_preds, aes(preds_full, observed)) + geom_point() 

# pl_full +
#   geom_point(aes(preds_prune, observed), shape = 2)
```

```{r}
#| echo: false
mse_full <- mean((df_mp_preds$observed - df_mp_preds$preds_full)^2)

mse_prune <- mean((df_mp_preds$observed - df_mp_preds$preds_prune)^2)
```

- MSE for pruned: `r round(mse_prune, 3)`
- MSE for full: `r round(mse_full, 3)`

\pause 

So, pruning helped us avoid some overfitting. 

\pause 

(Typical pred error of $\sqrt{`r round(mse_prune, 3)`} \approx `r round(sqrt(mse_prune), 3)`$)

\pause 
(Bigger than IQR of `r round(IQR(mps$ln.net), 3)`, but range covers $\left[ `r round(min(mps$ln.net), 2)`, `r round(max(mps$ln.net), 2)`
 \right]$.)
 
 \pause 
(Pretty good for 1 split!?)

## Random Forests



# Heterogeneous Treatment Effects

# Variable Selection

## Slide Title

Material.

<!-- Include a PDF/PNG/... -->
<!-- ![](figs/myfig.pdf){fig-align="center" height=80%} -->

<!-- Include PDFs in columns -->
<!-- \includegraphics[width=2in]{figs/myfig1.pdf} -->
<!-- \includegraphics[width=2in]{figs/myfig2.pdf} -->

***

\huge

\begin{center}
Thanks! 
\end{center}

\vspace{5mm}

\large

\center
`rtm@american.edu`  
`www.ryantmoore.org`  

## References {.allowframebreaks}

\footnotesize

