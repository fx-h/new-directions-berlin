---
title: | 
    | Data Science for Causal Inference
author: "Ryan T. Moore"
date: 2024-07-15
date-format: iso
execute: 
  echo: true
format: 
  beamer:
    fonttheme: serif
    include-in-header:
      - text: | 
          \usepackage{amsmath}
          \usepackage{wasysym}
          \newcommand{\independent}{\perp\mkern-9.5mu\perp}
    section-titles: true
    toc: true
institute:
  - American University
  - The Lab @ DC
bibliography: "../admin/main.bib"
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| results: false
#| warning: false

library(broom)
library(dagitty)
library(estimatr)
library(ggdag)
library(here)
library(tidyverse)
```

# Introductions

## About Me

\Large

- Associate Prof of Government  
(American University)
- Associate Director, Center for Data Science  
(American University)
- Senior Social Scientist  
(The Lab @ DC)
- Fellow in Methodology  
(US Office of Evaluation Sciences: "OES") \pause 
- Research agenda: political methodology, causal inference, experimental design, 
experiments in public policy

## About You!

\Large

>- Name?
>- Role? 
>- Interests?
>- Olympic sport you look forward to?

## Plan

\large

>- Data Science in Causal Inference
>    - Models
>    - Heterogeneous treatment effects
>    - Variable selection
>- Sensitivity
>    - Model specification
>    - Unobservable parameter
>    - Unobserved confounders
>- Modern difference-in-difference designs
>    - Canonical DiD
>    - Multiple time periods
>    - Staggered adoption
>    - Calloway-Sant'Anna approach



# Data Science in Causal Inference

## Causal Inference Approaches

The "potential outcomes" framework:

\pause 

\begin{center}
\begin{tabular}{ccccc}
 & & Would Enroll if & Would Enroll if  & \\
Citizen & Canvass? & Canvass? & No Canvass? & Enroll \\ \hline
1 & Yes & \only<2->{Yes} & \only<6->{\alert{(Yes)}} &  Yes \\
2 & Yes & \only<3->{Yes} & \only<6->{\alert{(No)}} &  Yes \\
3 & No & \only<6->{\alert{(Yes)}} & \only<4->{No} &  No \\
4 & No &\only<6->{\alert{(No)}} & \only<5->{No} &  No
\end{tabular}
\end{center}

## Causal Inference Approaches

The "potential outcomes" framework, more abstractly:

\begin{center}
\begin{tabular}{cccccc}
&&&&& True $\tau$ \\
Unit $i$ & Treatment $T$ & $Y(1)$ & $Y(0)$ & $Y^{\text{obs}}$ & $Y(1) - Y(0)$ \\ \hline
1 & 1 & 10 & \only<2->{\alert{(10)}} &  10 & \only<2->{0}\\
2 & 1 & 20 & \only<2->{\alert{(10)}} &  20 & \only<2->{10}\\
3 & 0 & \only<2->{\alert{(40)}} & 15 &  15 & \only<2->{25}\\
4 & 0 & \only<2->{\alert{(20)}} & 5 &  5 & \only<2->{15} \\ \hline
&&&& \only<3->{ATE $=\bar{\tau}=$} & \only<3->{$\frac{50}{4} = 12.5$} \\
&&&& \only<4->{$\widehat{ATE}=\hat{\bar{\tau}}=$} & \only<4->{$15 - 10 =5$} \\
\end{tabular}
\end{center}

## Causal Inference Approaches

\large

The "potential outcomes" framework, notation:

- Units indexed by $i$
- Treatment $T_i$ or $D_i$ or $Z_i$
- Outcome if treated $Y_i(1)$
- Outcome if control $Y_i(0)$
- True treatment effect $\tau_i = Y_i(1) - Y_i(0)$
- True average treatment effect $\bar{\tau} = \frac{1}{n} \sum_{i=1}^n \left(Y_i(1) - Y_i(0)\right)$
- Pre-treatment covariates $\mathbf X$

\pause 
(and we'll draw some DAG's, too)

## Data Science Approaches

\Large 

Three tasks of data science:

>- Description
>- Prediction
>- Causal Inference

\pause 
\vspace{1mm}

Models/algorithms central to all three.

\pause 
\vspace{1mm}

@herhsuhea19

## Data Science Approaches

\large 

Description

>- Identifying patterns, etc.
>- E.g., clustering to discover groups

## Data Science Approaches

\large 

Prediction

>- Components
>    - Inputs/outputs (predictors/outcomes, features/responses, \ldots)
>    - Mapping from inputs to outputs (linear model, decision tree, \ldots)
>    - Metric for evaluating mapping
>- With these, model machine learning does the work
>- E.g., regression, random forests, neural networks, \ldots

## Data Science Approaches

\large

Causal Inference

>- Potential outcomes/counterfactual/interventionist perspective
>- Requires _expertise_ different to description/prediction
>- Requires more than summary statistics, metrics, etc.
>- Requires some knowledge of causal structure
>    - Not all inputs treated same
>    - $T$ v. $\mathbf X$ -- very different!
>    - (the more knowledge, the better!)
>    - (alternative: solve fundamental problem of causal inference!)
>- E.g., experiments, observational causal designs, \ldots

## Causal Inference with Machine Learning

\pause 

![Don't do this.](figs/01-ml-ols-tweet.png)
\pause 

(OK, not "machine learning", perhaps, but _models_ at least \ldots)

## Causal Inference with Models

```{r}
#| echo: false
library(quartets)

data("causal_confounding")
df1 <- causal_confounding

data("causal_collider")
df2 <- causal_collider
```

Loaded two datasets:

```{r}
str(df1)
str(df2)
```

## Causal Inference with Models

```{r}
#| echo: false
#| layout-ncol: 2

p1 <- ggplot(df1, aes(exposure, outcome)) + 
  geom_point(aes(color = covariate)) + geom_smooth(method = "lm") +
  labs(title = "Data set 1")

p2 <- ggplot(df2, aes(exposure, outcome)) + 
  geom_point(aes(color = covariate)) + geom_smooth(method = "lm") +
  labs(title = "Data set 2")

p1
p2
```

## Causal Inference with Models

Model each

```{r}
lm_df1 <- lm(outcome ~ exposure, data = df1)
lm_df2 <- lm(outcome ~ exposure, data = df2)
```

```{r}
#| echo: false

tidy_df1 <- tidy(lm_df1) |> mutate(data = "df1", .before = "term") |> select(data:std.error)
tidy_df2 <- tidy(lm_df2) |> mutate(data = "df2", .before = "term") |> select(data:std.error)
tidy_df12 <- bind_rows(tidy_df1, tidy_df2)
tidy_df12
```

\pause 

>- Both cases: effect of exposure $\approx 1$. 
>- Is this good?
>- What if we adjust for covariate?

## Causal Inference with Models

```{r}
lm_df1_adj <- lm(outcome ~ exposure + covariate, data = df1)
lm_df2_adj <- lm(outcome ~ exposure + covariate, data = df2)
```

```{r}
#| echo: false

tidy_df1_adj <- tidy(lm_df1_adj) |> mutate(data = "df1", .before = "term") |>
  filter(term != "(Intercept)") |> select(data:std.error)
tidy_df2_adj <- tidy(lm_df2_adj) |> mutate(data = "df2", .before = "term") |> 
  filter(term != "(Intercept)") |> select(data:std.error)
tidy_df12_adj <- bind_rows(tidy_df1_adj, tidy_df2_adj)
tidy_df12_adj
```

>- Both cases: effect of exposure $\approx 0.5$. 
>- Is this good?
>- Which is correct? $\beta = 1$? $\beta = 0.5$?
>- _Should_ we adjust for covariate?

## Causal Inference with Models

\Large
There is nothing in the data that tells us. \pause \frownie

\pause 

Here are the true structures:

```{r quartetdags}
#| echo: false
#| layout-ncol: 3
coords <- list(x = c(T = 0, X = 1, Y = 2), 
               y = c(T = 1, X = 0, Y = 1.1))
dag_conf <- dagify(Y ~ T, T ~ X, Y ~ X, coords = coords) 
ggdag_conf <- dag_conf |> 
  ggdag_classic(size = 30) + theme_dag_blank() 

coords <- list(x = c(T = 0, X = 1, Y = 2), 
               y = c(T = 1, X = 0, Y = 1.1))
dag_coll <- dagify(Y ~ T, X ~ T, X ~ Y, coords = coords) 
ggdag_coll <- dag_coll |> 
  ggdag_classic(size = 30) + theme_dag_blank()

ggdag_conf
ggplot(NULL) + theme(panel.background = element_blank())
ggdag_coll
```

## Causal Inference with Models

When know structures, adjustment sets for unbiasedness differ:

- `df1`: confounding $\Rightarrow$ **adjust for $X$**
- `df2`: collider $\Rightarrow$ **do not adjust for $X$**

```{r}
g_conf <- dagitty("dag{ x -> y ; x <- c -> y }")
g_coll <- dagitty("dag{ x -> y ; x -> c <- y }")

adjustmentSets(g_conf, "x", "y")
adjustmentSets(g_coll, "x", "y")
```

## Causal Inference with Models

When know structures, adjustment sets for unbiasedness differ:

- `df1`: confounding $\Rightarrow$ **adjust for $X$**
- `df2`: collider $\Rightarrow$ **do not adjust for $X$**

```{r quartetdagadjustments}
#| echo: false
#| layout-ncol: 2

dag_conf |> ggdag_adjustment_set(exposure = "T", outcome = "Y")
dag_coll |> ggdag_adjustment_set(exposure = "T", outcome = "Y")
```

\pause 

(Data from @dagostino23)

## Causal Inference with Models

\large

>- Importance of identifying "pre-treatment covariates", "proper covariates"; doing "design before analysis"
>- Importance of experiments: strong knowledge about (part of) causal structure
>- Causal inference is critical to scientific questions, and separate from prediction
>- Though, methods from prediction can aid causal inference
>- (A perspective on "causal euphimisms": @hernan18)


## Approaches of Prediction and Causal Inference

\large

_Two Cultures_, [@breiman01-two]

- _Data Models_: our "social science modeling"
- _Algorithmic Models_: our "data science algorithms"

## Methods for Prediction and Causal Inference

\large

- Cross-validation
- Regression/Decision trees
- Random forests

@jamwithas21

## Cross-validation

\large

$k$-fold cross-validation

- Randomly partition data into $k$ groups
- Apply method to $k-1$ groups
- Use result to predict for left-out group
- Calculate $\text{MSE}_i = \frac{1}{n} \sum\limits_{i=1}^n \left(y_i - \hat{y}_i \right)^2$ 
- Calculate test error as average of the $k$ MSE's:

$$CV_{(k)} = \frac{1}{k} \sum\limits_{i=1}^k \text{MSE}_i$$

- Select model that minimises $CV_{(k)}$


## CV for Linear Model

```{r}
library(tidyverse)

## Make data

mk_data <- function(n = 90, n_folds = 10){
  
  df <- tibble(
    x1 = rnorm(n),
    x2 = rnorm(n), 
    x3 = rnorm(n), 
    y = 0.1 * x1 + 0.2 * x2 + 0.5 * x3 + rnorm(n),
    cv_fold = sample(rep(1:n_folds, (n / n_folds)))
  )
  
}

df <- mk_data()
```

## CV for Linear Model

```{r}
head(df)
```

\pause 

```{r}
table(df$cv_fold)
```


## CV for Linear Model

```{r}
cv_lm <- function(data, fmla){
  
  n_folds <- max(data$cv_fold)
  store_mses <- vector("numeric", length = n_folds)
  
  for(idx in 1:n_folds){
    
    df_train <- data |> filter(cv_fold != idx)
    df_test <- data |> filter(cv_fold == idx)
  
    lm_out <- lm(fmla, data = df_train)
  
    predictions <- predict(lm_out, newdata = df_test)
    
    store_mses[idx] <- mean((df_test$y - predictions)^2)}

  test_error_cv_k <- mean(store_mses)
  return(test_error_cv_k)
}
```

## CV for Linear Model

\large

```{r}
cv_lm(data = df, fmla = y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2)
```

\pause 

```{r}
df <- mk_data()
cv_lm(df, y ~ x1 + x2 + x3)
```

## CV for Linear Model

```{r}
#| echo: false
#| fig-cap: "MSE always less (better) for 3-variable model."
#| fig-height: 6

set.seed(631537784)

n_iter <- 100

mses_2var <- mses_3var <- vector("numeric", length = 100)

for(idx in 1:n_iter){
  df <- mk_data()
  mses_2var[idx] <- cv_lm(df, y ~ x1 + x2)
  mses_3var[idx] <- cv_lm(df, y ~ x1 + x2 + x3)
}

mses <- tibble(mses_2var, 
               mses_3var, 
               difference = mses_3var - mses_2var)

ggplot(mses, aes(difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "(MSE 3-var model) - (MSE 2-var model). 100 data sets.")

# Long version:
# mses <- mses |> pivot_longer(cols = starts_with("mses"),
#                              names_to = "model") |>
#   mutate(model = case_when(model == "mses_2var" ~ "y ~ x1 + x2", TRUE ~ "y ~ x1 + x2 + x3"))

# ggplot(mses, aes(value, model)) + 
#  geom_violin(draw_quantiles = c(.25, .5, .75))
```



## Regression Trees

\large

- Partition predictor space into regions $R_1, R_2, \ldots, R_J$.
- If unit falls in region $R_j$, use average outcome in $R_j$ as predicted value: $\hat{y}_{R_j}$
- (For _decision_ on discrete outcome, count votes in $R_j$)
- Goal: minimise residual sum of squares (RSS), just like LS regression:

$$\sum\limits_{j=1}^J \sum\limits_{i \in R_j} \left(y_i - \hat{y}_{R_j}\right)$$

## Regression Trees

\large


How to define regions $R_j$?

\pause 

- Top-down, greedy recursive binary split
- At each step, find predictor and cut-point that minimise

$$\sum\limits_{i: x\in R_1(j,s)} \left(y_i - \hat{y}_{R_1(j,s)}\right)^2 +
\sum\limits_{i:x \in R_2(j,s)}  \left(y_i - \hat{y}_{R_2(j,s)}\right)^2$$

## Regression Trees

\Large

>- Overfitting is a potential problem 
>- Can we increase predictive quality by only using _part_ of a tree?
>- "Pruning"

## Regression Trees

Pruning

>- Build a large tree
>- Select the subtree that gives least prediction error (via cross-validation)
>- But, many possible subtrees, so penalise larger trees via $\alpha$
>- $\alpha$: penalty parameter
>- $|T|$: count of terminal nodes of $T$
>- $m$: terminal node index
>- Find subtree that minimises \pause 

$$\sum\limits_{m=1}^{|T|} \sum\limits_{i:x_i \in R_m} \left(y_i - \hat{y}_{R_m} \right)^2 + \alpha |T|$$
\pause 

Sum squared pred. error (plus penalty that grows with tree size) across units in region, then regions. 

## Regression Trees

\large

But, how to choose $\alpha$? (Use cross-validation.)

1. Build big tree on training data (with some minimum terminal node size) \pause 
2. For several values of $\alpha$, find best subtree. \pause 
3. Find best value of $\alpha$ via CV. Create $K$ folds. Then \pause \newline
3a. Do 1 and 2 on all but $k$th fold  \pause  
\newline 
3b. Predict for $k$th fold, calculate MSE for several values of $\alpha$  \pause  \newline
3c. Get avg MSE for each $\alpha$  \pause \newline
3d. Pick $\alpha$ to minimise MSE  \pause \newline
4. Using that $\alpha$, select best subtree from Step 2

## Example: Regression Tree

```{r}
library(qss)
library(rsample)
library(tree)

data("MPs")
mps <- MPs |> mutate(age = yod - yob,
                     is_labour = if_else(party == "labour", 1, 0),
                     is_london = if_else(region == "Greater London", 1, 0), 
                     is_winner = if_else(margin > 0, 1, 0)) |>
  select(ln.net, age, is_labour, is_london, is_winner) |> 
  na.omit()

set.seed(765076184)

mp_split <- initial_split(mps, prop = 0.7)

mp_train <- training(mp_split)
mp_test <- testing(mp_split)

tree_mp <- tree(ln.net ~ ., data = mp_train)
```

## Example: Regression Tree

```{r}
#| fig-cap: "The regression tree (for training data)"

plot(tree_mp)
text(tree_mp)
```

## Example: Regression Tree

\large
Would pruning help?

```{r}
#| fig-cap: "Subtree size 2 minimises SSR"
#| fig-height: 4
cv_mps <- cv.tree(tree_mp, K = 10)

plot(cv_mps$size, cv_mps$dev, type = "b")
```

\pause 
Yes. So prune at $|T| = 2$.

## Example: Regression Tree

```{r}
#| fig-cap: "The pruned tree"
#| fig-height: 3

prune_mps <- prune.tree(tree_mp, best = 2)

plot(prune_mps)
text(prune_mps)
```

## Example: Regression Tree

\large

Predict for test set:

```{r}
#| echo: false
mps_preds_full <- predict(tree_mp, newdata = mp_test)
mps_preds_prune <- predict(prune_mps, newdata = mp_test)

df_mp_preds <- tibble(
  preds_full = mps_preds_full,
  preds_prune = mps_preds_prune,
  observed = mp_test$ln.net
)

# pl_full <- ggplot(df_mp_preds, aes(preds_full, observed)) + geom_point() 

# pl_full +
#   geom_point(aes(preds_prune, observed), shape = 2)
```

```{r}
#| echo: false
mse_full <- mean((df_mp_preds$observed - df_mp_preds$preds_full)^2)

mse_prune <- mean((df_mp_preds$observed - df_mp_preds$preds_prune)^2)
```

- MSE for pruned: `r round(mse_prune, 3)`
- MSE for full: `r round(mse_full, 3)`

\pause 

So, pruning helped us avoid some overfitting. 

\pause 

(Typical pred error of $\sqrt{`r round(mse_prune, 3)`} \approx `r round(sqrt(mse_prune), 3)`$)

\pause 
(Bigger than IQR of `r round(IQR(mps$ln.net), 3)`, but range covers $\left[ `r round(min(mps$ln.net), 2)`, `r round(max(mps$ln.net), 2)`
 \right]$.)
 
 \pause 
(Pretty good for 1 split!?)

## Random Forests

\large

Next: random forest algorithm

\pause 

Ensemble learning algorithms:

\pause 

>- Boosting: models build on prior models $\rightsquigarrow$ pick feature, predict, upweight mispredicted data, .... Do several times and combine.
>- Bagging: (random select units, model) $\to$ many times. No building.

\pause 
Random Forests are bagging algorithms.

\pause 

_Bagging_: \textit{b}ootstrap \textit{agg}regation

## Random Forests

\Large
Why bag?

\pause 

>- Trees are low bias, high variance  
(diff answers, depend on data split)
>- Bagging averages over data subsets, reducing variance
>- (Linear regression: lower variance)

## Random Forests

\Large
Random forests: decorrelated, bagged trees

\pause 

>- Take bootstrapped training subsample
>- Build deep tree. At each split, _randomly sample_ $m$ of $p$ predictors, build split from only those $m$.
>- (Often choose $m \approx \sqrt{p}$)
>- So, different splits consider different predictors
>- So, trees will look very different to each other

## Example: Random Forests

\large

```{r}
library(randomForest)

# Full bag:
bag_mps <- randomForest(ln.net ~ ., data = mp_train,
                        ntree = 500, mtry = 4, 
                        importance = TRUE)

# Decorrelate:
rf_mps <- randomForest(ln.net ~ ., data = mp_train,
                        ntree = 500, mtry = 2, 
                       importance = TRUE)
```

## Example: Random Forests

\large

Predict:
```{r}
preds_bag <- predict(bag_mps, newdata = mp_test)

preds_rf <- predict(rf_mps, newdata = mp_test)
```

```{r}
#| echo: false

mse_bag <- mean((mp_test$ln.net - preds_bag)^2)

mse_rf <- mean((mp_test$ln.net - preds_rf)^2)
```

- MSE for RF: `r round(mse_rf, 3)`
- MSE for full bag: `r round(mse_bag, 3)`

\pause 

So, decorrelating helped us avoid some overfitting to each bootstrap subsample (and thus, reduced variance).

\pause 

(Typical pred error of $\sqrt{`r round(mse_rf, 3)`} \approx `r round(sqrt(mse_rf), 3)`$)

\pause 
(Bigger than IQR of `r round(IQR(mps$ln.net), 3)`, but range covers $\left[ `r round(min(mps$ln.net), 2)`, `r round(max(mps$ln.net), 2)`
 \right]$.)

# Heterogeneous Treatment Effects

## Homogeneous and Heterogeneous Effects

\large

>- Most causal inference starts at _average treatment effects_
>- Average may be interesting on its own, \ldots \pause but often masks assumption of _homogeneous_ effects \pause 
>- Notationally, often assume $\tau_i = \tau \quad \forall i$
>- But, _heterogeneous_ effects often of central interest
>- Different effects for different groups
>    - Subgroup variability (research)
>    - Targeting resources (campaigns, marketing)
>    - Constituency effects (public policy)
>- Notationally, $\exists i: \tau_i \neq \tau$

## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \epsilon$$

\pause 

```{r}
lm_out <- lm(ln.net ~ is_winner, data = mps)
lm_out
```

## Homogeneous and Heterogeneous Effects: Estimation

\large 

Homogeneous effects:

```{r}
t.test(ln.net ~ is_winner, data = mps)
```


## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \sum \beta_j X_j +  \epsilon$$

\pause 

```{r}
lm_out <- lm(ln.net ~ is_winner + is_labour + 
               is_london + age, data = mps)
lm_out
```

## Homogeneous and Heterogeneous Effects: Estimation

Homogeneous effects:

```{r}
lm_lin(ln.net ~ is_winner, covariates = ~ is_labour + is_london + age, data = mps)
```



## Homogeneous and Heterogeneous Effects: Detection

Heterogeneous effects:

## CATEs: Conditional ATEs

\Large

>- _Conditional average treatment effect_ (CATE): avg treatment effect for subset of population
>- Sometimes "CACE"
>- Inference: not "evidence against $\text{TE} = 0$?", but "evidence against $\text{CATE}_1 = \text{CATE}_2$?"

## Homogeneous and Heterogeneous Effects: Estimation

Heterogeneous effects:

$$\text{Outcome} = \beta_0 + \beta_1 \text{Treatment} + \beta_2 \text{Group} + \beta_3 \text{Treatment} \cdot \text{Group}  + \epsilon$$

\pause 
>- $\beta_1$ gives TE for `Group == 0` 
>- $\beta_1 + \beta_3$ gives TE for `Group == 1` 

## Homogeneous and Heterogeneous Effects: Estimation

Heterogeneous effects:

```{r}
lm_out <- lm(ln.net ~ is_winner * is_labour + 
               is_london + age, data = mps)
coef(lm_out) |> round(3)
```


## Causal Forests

\large

>- Our regression trees had terminal nodes ("leaves") that were sufficiently homogeneous for prediction.
>- Use $\hat{y}_{R_j}$ as pred value for obs in $R_j$
>- (Tory, winner, London $\to 13.84$)
>- $$\hat{y}_{R_j} = \frac{1}{|R_j|} \sum\limits_{i \in R_j} Y_i$$

## Causal Forests

\large

>- Similarly, consider each leaf $R_j$ small, homogeneous enough that potential outcomes independent
>- Treateds in $R_j$ provide good estimates of what controls in $R_j$ would have done under treatment
>- Controls in $R_j$ provide good estimates of what treateds in $R_j$ would have done under control
>- Assignment w/in leaf $R_j$ is as-good-as-random
>- I.e., each leaf contains an experiment

\pause 

$$Y(0), Y(1) \independent T | \bf X $$

## Causal Forests

\large

>- Let $\{T, R_j\} = \{i: T_i = 1, i \in R_j \}$ \quad (Tr obs in $R_j$)
>- Let $\{C, R_j\} = \{i: T_i = 0, i \in R_j \}$ \quad (Co obs in $R_j$)
>- Natural estimation of \pause 

$$\hat{\bar{\tau}}_{R_j} = \frac{1}{|\{T, R_j\}|} \sum\limits_{\{T, R_j \}} Y_i -  \frac{1}{|\{C, R_j\}|} \sum\limits_{\{C, R_j \}} Y_i$$

\pause 
So, we can use RF methods to estimate conditional (heterogeneous) treatment effects, CATEs.

\pause 

\center
\smiley


## Causal Forests: Honesty

\large

>- But need one more thing for asymptotics to work out \ldots
>- Each tree must be _honest_
>- Each unit $i$ _either_ 
>    - used to determine tree splits, _or_
>    - used to estimate $\hat{\bar{\tau}}_{R_j}$
>- But **not both**!
>- One way: "Double-sample" causal trees
>    - Split training data into $\mathcal{I}$ and $\mathcal{J}$
>    - Splits chosen to maximise variance on $\hat{\bar{\tau}} \text{ for } i \in \mathcal{J}$
>    - Splitting cannot use $y_i$ from $\mathcal{I}$
>    - Prediction, estimation of $\hat{\bar{\tau}}$ uses only $\mathcal{I}$
>- Build a random forest (decorrelated deep trees picking from $m$ predictors) of causal trees

## Example: Causal Forests

```{r}
#| eval: true

library(grf)

X <- mp_train |> select(age, is_labour, is_london)

W <- mp_train |> select(is_winner) |> 
  unlist() |> as.numeric() 

Y <- mp_train |> select(ln.net) |> unlist() 

cf_out <- causal_forest(X, Y, W)
```

## Example: Causal Forests

\large 

```{r}
X_test <- mp_test |> select(age, is_labour, is_london)

cf_pred_est_var <- predict(cf_out, X_test, 
                           estimate.variance = TRUE)

cf_preds <- cf_pred_est_var$predictions

df_cf <- tibble(X_test, 
                cf_te = cf_preds,
                cf_se = sqrt(cf_pred_est_var$variance.estimates),
                te_1se_lower = cf_te - cf_se,
                te_1se_upper = cf_te + cf_se)
```

## Example: Causal Forests Results, Party

```{r}
#| echo: false
#| fig-height: 6

ggplot(df_cf, aes(factor(is_labour), cf_te)) + 
  geom_violin(draw_quantiles = (1:3) / 4) +
  labs(x = "Is Labour?", y = "Honest CF Estimated Treatment Effect")

tory_mean_cf_te <- mean(df_cf$cf_te[df_cf$is_labour == 0])

labour_mean_cf_te <- mean(df_cf$cf_te[df_cf$is_labour == 1])
```

\pause 
- Mean CF TE, Tory: `r round(tory_mean_cf_te, 3)` \pause $\rightsquigarrow \pounds `r formatC(round(exp(median(mps$ln.net) + tory_mean_cf_te) - exp(median(mps$ln.net)), -3), format = "f", digits = 0, big.mark = ",")`$ \pause 
- Mean CF TE, Labour: `r round(labour_mean_cf_te, 3)` \pause $\rightsquigarrow \pounds `r formatC(round(exp(median(mps$ln.net) + labour_mean_cf_te) - exp(median(mps$ln.net)), -3), format = "f", digits = 0, big.mark = ",")`$ 


## Example: Causal Forests Results, London

```{r}
#| echo: false

ggplot(df_cf, aes(factor(is_london), cf_te)) + 
  geom_violin(draw_quantiles = (1:3) / 4) +
  labs(x = "Is London?", y = "Honest CF Estimated Treatment Effect")
```

## Example: Causal Forests Results, Age

```{r}
#| echo: false

ggplot(df_cf, aes(x = age, y = cf_te)) + 
  geom_point(aes(colour = factor(is_labour))) + 
  geom_smooth(aes(colour = factor(is_labour)), se = FALSE) + 
  labs(x = "Age at Death", y = "Honest CF Estimated Treatment Effect")
```

## Example: Causal Forests Results, Age

```{r}
#| echo: false

ggplot(df_cf, aes(x = age, y = cf_te)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = te_1se_lower, 
                    ymax = te_1se_upper)) +
  geom_smooth(se = FALSE) + 
  labs(x = "Age at Death", y = "Honest CF Estimated Treatment Effect")
```



# Variable Selection

## Feature Selection

>- Wrappers: pick subset of covars, train on data (estimate model), test on hold-out, score predictions.  Keep best-scoring subset.
>- Filters: correlate covars with outcome.  Keep strongest.
>- Embeds: select features and estimate model at same time.  Penalize using more predictors.

## Regularization Methods

\large

OLS reminder

Minimize SSR: 

\begin{eqnarray*}
&& \text{argmin}_\beta \sum\limits_{i=1}^n \left(y_i - \hat{y}_i \right)^2 \\
&& \text{argmin}_\beta \sum\limits_{i=1}^n \left(\mathbf{y} - \mathbf{X}\hat{\beta} \right)^2
\end{eqnarray*}

## Embedded Regularization Methods

\large

L1 regularization: the LASSO (Least Absolute Shrinkage and Selection Operator)

\begin{eqnarray*}
&& \text{argmin}_\beta \left[ \sum\limits_{i=1}^n \left(y_i - \mathbf{X}\hat{\beta} \right)^2 + \lambda \sum\limits_{j = 1}^k |\hat{\beta}_j| \right]
\end{eqnarray*}

\pause 

L2 regularization: Ridge regression

\begin{eqnarray*}
\text{argmin}_\beta \left[ \sum\limits_{i=1}^n \left(y_i - \mathbf{X}\hat{\beta} \right)^2 + 
\lambda \sum\limits_{j = 1}^k \hat{\beta}_j^2 \right]
\end{eqnarray*}

## Embedded Regularization Methods

\large

Mix L1 and L2: Elastic net

\begin{eqnarray*}
\text{argmin}_\beta \left( \frac{\sum\limits_{i=1}^n \left(y_i - \mathbf{X}\hat{\beta} \right)^2}{2n} + 
\lambda \left[  \alpha \sum\limits_{j = 1}^k |\hat{\beta}_j| +
\frac{1-\alpha}{2} \sum\limits_{j = 1}^k \hat{\beta}_j^2  \right] \right)
\end{eqnarray*}


\pause 
Regularized trees, \ldots

## Embedded Regularization Methods

\large

How to choose $\lambda$, $\alpha$?

\pause 

## The LASSO

```{r}
#| echo: false
#| eval: false

predictors <- c("age", "is_labour", "is_london", "is_winner")

X <- mps[, predictors]
```

## The LASSO

Cross-validation for $\lambda$:

```{r}
#| label: cvlasso

df_lasso <- read_csv("~/Desktop/lasso.csv")

X <- as.matrix(df_lasso[, 2:ncol(df_lasso)])

Y <- as.matrix(df_lasso[, "y"])

library(glmnet)

cv_lasso <- cv.glmnet(X, Y, alpha = 1)
```

## The LASSO

```{r}
#| fig-height: 5
plot(cv_lasso)
cv_lasso$lambda.min
```

## The LASSO

Implement:

```{r}
lasso_out <- glmnet(X, Y, alpha = 1, 
                    lambda = cv_lasso$lambda.min)

lasso_out
```

## The LASSO

Coefficients:

```{r}
coef_lasso <- coef(lasso_out)
round(coef_lasso, 3)
```

## The LASSO

Coefficients:

```{r}
round(coef_lasso[, ], 3)
```


## The LASSO

Implement, alternative $\lambda$:

```{r}
lasso_1se <- glmnet(X, Y, alpha = 1, 
                    lambda = cv_lasso$lambda.1se)

coef(lasso_1se)
```

## The LASSO

Coefficients:

```{r}
round(coef(lasso_1se)[, ], 3)
```

## The Double LASSO for Treatment Effects

\large

The idea: 

- covariates may $\rightsquigarrow Y$ or $\rightsquigarrow T$ \pause 
- $\approx$ "double robust", "AIPW" estimators \pause 
- (different to just "doing LASSO twice" for regularization $+$ shrinkage)

## The Double LASSO for Treatment Effects

\large

1. Model $Y = f(X)$ using LASSO \pause 
2. Model $T = f(X)$, using LASSO \pause 
3. Let $X_{\text{LASSO}}$ be set of imp covariates identified s.t. each $\beta_{X_{\text{LASSO}}} > 0$ \pause 
4. Model $Y = T + X_{\text{LASSO}}$ \pause 



## The Double LASSO for Treatment Effects

```{r}
library(hdm)
```

```{r}
#| echo: false
#| eval: false

df_mps <- MPs |> mutate(age = yod - yob,
                        winner = as.numeric(margin > 0))

rlasso_out <- rlassoATE(ln.net ~ age + party + margin.pre + region + winner | age + party + margin.pre + region, data = df_mps)
```

```{r}
#| eval: true
data(social)

df_social <- social |> mutate(is_male = if_else(sex == "male", 1, 0),
                              age = 2006 - yearofbirth,
                              is_neighbors = if_else(messages == "Neighbors", 1, 0)) |>
  filter(messages %in% c("Neighbors", "Control"))

rlasso_out <- rlassoATE(primary2006 ~ age + is_male + primary2004 + hhsize + is_neighbors | age + is_male + primary2004 + hhsize, data = df_social)
```

## The Double LASSO for Treatment Effects

```{r}
summary(rlasso_out)
```

## The Double LASSO for Treatment Effects

```{r}
X <- as.matrix(df_social[, c("age", "is_male", "primary2004", "hhsize", "is_neighbors")])

Y <- as.matrix(df_social[, "primary2006"])

D <- as.matrix(df_social[, "is_neighbors"])

summary(rlassoEffects(X, Y, method = "double selection"))

```



## R packages for Regularization, etc.

\Large

- `glmnet`
- `caret`

See also `tidymodels`, `parsnip`, \ldots

## Embedded Regularization Methods



<!-- Include a PDF/PNG/... -->
<!-- ![](figs/myfig.pdf){fig-align="center" height=80%} -->

<!-- Include PDFs in columns -->
<!-- \includegraphics[width=2in]{figs/myfig1.pdf} -->
<!-- \includegraphics[width=2in]{figs/myfig2.pdf} -->

***

\huge

\begin{center}
Thanks! 
\end{center}

\vspace{5mm}

\large

\begin{center}
\texttt{rtm@american.edu} \\
\texttt{www.ryantmoore.org}
\end{center}


## References {.allowframebreaks}

\footnotesize

